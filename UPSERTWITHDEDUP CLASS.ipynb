{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1d7992-1495-4988-9db8-9c3278d8e65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>price</th><th>catagory</th><th>updatedDate</th></tr></thead><tbody><tr><td>1</td><td>iPhone</td><td>1000</td><td>electronics</td><td>2025-11-23T00:30:11.517Z</td></tr><tr><td>2</td><td>Macbook</td><td>2000</td><td>electronics</td><td>2025-11-23T00:30:11.517Z</td></tr><tr><td>3</td><td>Airpods</td><td>500</td><td>electronics</td><td>2025-11-23T00:30:11.517Z</td></tr><tr><td>4</td><td>Shirt</td><td>100</td><td>clothing</td><td>2025-11-23T00:30:11.517Z</td></tr><tr><td>5</td><td>Pants</td><td>150</td><td>clothing</td><td>2025-11-23T00:30:11.517Z</td></tr><tr><td>5</td><td>Trousers</td><td>150</td><td>clothing</td><td>2025-11-24T21:09:42.783Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "iPhone",
         1000,
         "electronics",
         "2025-11-23T00:30:11.517Z"
        ],
        [
         2,
         "Macbook",
         2000,
         "electronics",
         "2025-11-23T00:30:11.517Z"
        ],
        [
         3,
         "Airpods",
         500,
         "electronics",
         "2025-11-23T00:30:11.517Z"
        ],
        [
         4,
         "Shirt",
         100,
         "clothing",
         "2025-11-23T00:30:11.517Z"
        ],
        [
         5,
         "Pants",
         150,
         "clothing",
         "2025-11-23T00:30:11.517Z"
        ],
        [
         5,
         "Trousers",
         150,
         "clothing",
         "2025-11-24T21:09:42.783Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "price",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "catagory",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "updatedDate",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 54
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "catagory",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "updatedDate",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "select *from pyspark_cata.source.products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad211de0-1868-40bb-b2b1-5a122f5dc108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06db912-5ce6-442d-90ee-9f9d6be645d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Delta Table Created with schema\n"
     ]
    }
   ],
   "source": [
    "class upsertwithdedup:\n",
    "    def __init__(self, df):\n",
    "        self.df = df \n",
    "    \n",
    "\n",
    "    def dedup(self, keyCol, cdcCol):\n",
    "        df = self.df.withColumn(\"dedup\", row_number().over(window.partitionBy(keyCol).orderBy(desc(cdcCol))))\n",
    "        df = df.filter(col(\"dedup\"==1).drop(\"dedup\"))\n",
    "        #replace stored df\n",
    "        self.df = df\n",
    "        return df\n",
    "    \n",
    "    def table_exists(self, spark, targetPath):\n",
    "        try:\n",
    "            DeltaTable.forPath(spark, targetPath)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    def created_delta_table(self, spark, tartgetPath):\n",
    "        (\n",
    "            self.df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(targetPath)\n",
    "\n",
    "        )\n",
    "    print(\"New Delta Table Created with schema\")\n",
    "\n",
    "    \n",
    "    def upsert(self, spark, targetPath, keyCol, cdcCol): \n",
    "        if not self.table_exists(spark, targetPath):\n",
    "            print(\"Table does not exist, creating new table\")\n",
    "            self.created_delta_table (spark, targetPath)\n",
    "            return\n",
    "        print(\"Valid Delta table found -> permorning MERGE...\")\n",
    "\n",
    "        dlt_obj = DeltaTable.forPath(spark, targetPath)\n",
    "        (\n",
    "            dlt_obj.alias(\"trg\")\n",
    "            .merge(self.df.alias(\"src\")),\n",
    "            f\"src.{keyCol} = trg.{keyCol}\"\n",
    "        )\\\n",
    "        .whenMatchedUpdateAll(condition= f\"src.{cdcCol} >=trg.{cdcCol}\")\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "        print(\"Merge completed\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09dfe4e5-33f4-4428-8c36-b2d32e438fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#loading data frame\n",
    "df = spark.table(\"pyspark_cata.source.products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155223f9-c4d8-4919-9704-3d7222b68acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating class oject\n",
    "obj = upsertwithdedup(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9badbd0f-5c38-40ed-a009-16a05e0c9f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+direct_to_type_checking": {}
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8280771191677814>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_dedup \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mdedup(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdatedDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-8280771191677809>, line 7\u001B[0m, in \u001B[0;36mupsertwithdedup.dedup\u001B[0;34m(self, keyCol, cdcCol)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdedup\u001B[39m(\u001B[38;5;28mself\u001B[39m, keyCol, cdcCol):\n",
       "\u001B[0;32m----> 7\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(window\u001B[38;5;241m.\u001B[39mpartitionBY(keyCol)\u001B[38;5;241m.\u001B[39morderBy(desc(cdcCol))))\n",
       "\u001B[1;32m      8\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#replace stored df\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'partitionBY'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'function' object has no attribute 'partitionBY'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'function' object has no attribute 'partitionBY'"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KD00G",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-8280771191677814>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_dedup \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mdedup(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdatedDate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-8280771191677809>, line 7\u001B[0m, in \u001B[0;36mupsertwithdedup.dedup\u001B[0;34m(self, keyCol, cdcCol)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdedup\u001B[39m(\u001B[38;5;28mself\u001B[39m, keyCol, cdcCol):\n\u001B[0;32m----> 7\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m, row_number()\u001B[38;5;241m.\u001B[39mover(window\u001B[38;5;241m.\u001B[39mpartitionBY(keyCol)\u001B[38;5;241m.\u001B[39morderBy(desc(cdcCol))))\n\u001B[1;32m      8\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdedup\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#replace stored df\u001B[39;00m\n",
        "\u001B[0;31mAttributeError\u001B[0m: 'function' object has no attribute 'partitionBY'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dedup = obj.dedup(\"id\", \"updatedDate\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8280771191677810,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "UPSERTWITHDEDUP CLASS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}